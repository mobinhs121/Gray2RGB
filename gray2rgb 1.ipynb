{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnrya_th6xaE"
      },
      "outputs": [],
      "source": [
        "# # Set up Kaggle API key\n",
        "import os\n",
        "# Assuming you've uploaded your kaggle.json file\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n",
        "\n",
        "# # Download and unzip the \"img-net-10percent\" dataset\n",
        "!kaggle datasets download -d kerrit/imagenet1kmediumtest-10k\n",
        "!unzip imagenet1kmediumtest-10k\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "tf.config.run_functions_eagerly(True)\n",
        "# Define paths and directories\n",
        "data_dir = \"/content/test_10000/test_10000\"  # Path to the image folder\n",
        "\n",
        "# List subdirectories in the dataset directory\n",
        "subdirectories = os.listdir(data_dir)\n",
        "\n",
        "# Load and preprocess the grayscale images\n",
        "images = []\n",
        "image_shape = (256,256)"
      ],
      "metadata": {
        "id": "X5DO6L9VHROt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for subdirectory in subdirectories:\n",
        "    subdirectory_path = os.path.join(data_dir, subdirectory)\n",
        "    image_files = os.listdir(subdirectory_path)\n",
        "\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(subdirectory_path, image_file)\n",
        "        image = Image.open(image_path).convert('L')  # Load and convert to grayscale\n",
        "        image = image.resize(image_shape)  # Resize to the specified dimensions\n",
        "        image = np.array(image)\n",
        "\n",
        "        images.append(image)\n",
        "\n",
        "images = np.stack(images)  # Convert the list of images to a NumPy array\n",
        "images = images.reshape(images.shape[0], *image_shape, 1)\n",
        "\n",
        "# Create an ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1.0/255)  # You can add more augmentation settings\n",
        "\n",
        "# Create a data generator\n",
        "batch_size = 16\n",
        "data_generator = datagen.flow(images, images, batch_size=batch_size)\n",
        "\n",
        "# Create the model (same as before)\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 1)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(UpSampling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='mse')  # Mean squared error loss for regression task\n",
        "\n",
        "# Train the model using the data generator\n",
        "steps_per_epoch = len(images) // batch_size\n",
        "\n"
      ],
      "metadata": {
        "id": "xQqz85NaHJh2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='mse')  # Mean squared error loss for regression task\n",
        "\n",
        "# Train the model using the data generator\n",
        "steps_per_epoch = len(images) // batch_size\n",
        "model.fit(data_generator, steps_per_epoch=steps_per_epoch, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpmBG-UNHNMz",
        "outputId": "0bbed805-f134-4473-cf51-4cc9c41b52b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ]
    }
  ]
}